---
title: "Hw1"
output: html_document
date: "2025-09-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# General Information

Github Repo Link: "https://github.com/R-4-Data-Science/HW1"

Logan Bolton ChatGPT Link: "https://chatgpt.com/share/68d4c5be-d4e0-800e-971d-5e981b7c0f8f"
Jonah Kennedy ChatGPT Link: "https://chatgpt.com/c/68d4d9b3-1480-8330-8fda-e4c97592cc79"

# 2. Vectors & Factors
## 2.1 Named numeric vector, attributes

typeof(wts) returns a double vector, length(wts) returns 5, names(wts) shows a–e, and attributes(wts) confirms the custom units = "kg" attribute.

```{r}
wts <- c(10, 20, 30, 40, 50)
names(wts) <- c("a", "b", "c", "d", "e")

attr(wts, "units") <- "kg"

typeof(wts)
length(wts)
names(wts)
attributes(wts)
```

## 2.2 Special values & coercion

By default, R will coerce mixed data types of numbers and characters into only being characters. That's why `typeof(v)` returns the value "character".

```{r}
v <- c(1, NA, NaN, Inf, "2")
typeof(v)
```

as.numeric(v) makes numeric values, median(finite_vals) is 1.5, and replacing non-finite entries yields v_clean where NA, NaN, and Inf are all set to 1.5.

```{r}
v_num <- as.numeric(v)
finite_vals <- v_num[is.finite(v_num)]
median_val <- median(finite_vals)
print(median_val)

v_clean <- v_num
v_clean[!is.finite(v_clean)] <- median_val
v_clean[is.na(v_clean)] <- median_val
v_clean
```
## 2.3 Factors

In the mtcars dataset, 0 is automatic and 1 is manual. In the displayed table, there are 19 automatics and 13 manuals.

```{r}
table(mtcars$am)
am_f <- factor(mtcars$am, 
               levels = c(0, 1), 
               labels = c("Automatic", "Manual"))
am_f
```
```{r}
am_f <- relevel(am_f, ref = "Manual")
levels(am_f)
table(am_f)

```
# 3 Data Frames & Coercion
## 3.1 Data frame construction

print(df_small) shows the intended columns. sapply(df_small, class) confirms that id is an integer, group is a factor, and val is double with one NA value.

```{r}
id <- 1:6
group <- factor(c("A", "B", "A", "B", "A", "B"))
val <- c(10.5, 20.1, NA, 15.2, 30.0, 25.3)

df_small <- data.frame(id = id, group = group, val = val)
print(df_small)
sapply(df_small, class)

```


## 3.2 Coercion rules (vector vs data frame)

When we bind the dataframes together using `c(...)`, it converts everything to a double. This is because R stores the values internally as integers. Then when concatenating the values, everything is converted to the same numeric type and the group labels are lost. 

To prevent this from happening, we can create a new dataframe that combines these vectors together with binding. This can be accomplished with the regular `data.frame(...)` function.

```{r}
concatted = c(df_small$group, df_small$val)
print(concatted)
typeof(concatted)
```
```{r}
df_pair <- data.frame(group = df_small$group,
                      val   = df_small$val)
print(df_pair)
print(typeof(df_pair))
```


# 4 Subsetting (12 pts)

`head(y)` prints the first six mpg values. `head(Xdf)` shows all other columns in a data frame.

```{r}
y <- mtcars$mpg
Xdf <- mtcars[, !(names(mtcars) %in% "mpg")]

head(y)
head(Xdf)
```

## 4.1 By position, name, logical

`Xdf[, 1:3]` selects the first three columns by position.

`Xdf[, c("hp","wt","qsec")]` selects named columns in that order.

`Xdf[Xdf$wt > 3, ]` filters rows with weight greater than 3.

### By Position
```{r}
Xdf_pos <- Xdf[, 1:3]
head(Xdf_pos)
```

### By Name 
```{r}
Xdf_names <- Xdf[, c("hp", "wt", "qsec")]
head(Xdf_names)
```

### By Logical Condition 
```{r}
Xdf_wt3 <- Xdf[Xdf$wt > 3, ]
head(Xdf_wt3)
```
## 4.2 Negative indices via matching

Dropping vs and am leaves an 8-column data frame, which is confirmed by `ncol(Xdf_drop)`.

```{r}
Xdf_drop <- Xdf[, -match(c("vs","am"), names(Xdf))]
head(Xdf_drop)

ncol(Xdf_drop)
```

## 4.3 List subsetting differences

L[["y"]] returns the object itself (a numeric vector). L["y"] returns a one-element list that still wraps the vector. L$y is equivalent to L[["y"]] for a named element.

```{r}
L <- list(
  y = y,           
  X = Xdf,        
  meta = list(n = nrow(Xdf))
)

class(L[["y"]])

class(L["y"])

class(L$y)

```




# 5 Matrices & Arrays (18 pts)

## 5.1 Model matrix with intercept

`cbind(Intercept = 1, as.matrix(...))` creates a model matrix with a column of ones plus wt and hp; `dim(X)` is 32×3 and `colnames(X)` confirms the column names.

```{r}
S <- c("wt","hp")
#Binds a vector of 1s and the given vectors of S into a matrix  
X <- cbind(Intercept = 1, as.matrix(Xdf[, S, drop = FALSE]))  #drop=FALSE keeps the subset as a data frame
dim(X)
colnames(X)

```

## 5.2 Least Squares via matrix operations

`beta_hat` matches the lm coefficients, `y_hat` are the fitted values, and `RSS` equals `sum(resid(lm_fit)^2)`, which verifies the correctness of the matrix solution.

```{r}
#Compute beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta_hat

#Compute fitted values
y_hat <- X %*% beta_hat
head(y_hat) 

#Compute RSS
RSS <- sum((y - y_hat)^2)
RSS

#Fit model
lm_fit <- lm(mpg ~ wt + hp, data = mtcars)
summary(lm_fit) #matches beta hat
sum(resid(lm_fit)^2) #matches RSS
```

## 5.3 AIC

We first used the simplified Gaussian AIC, `n*log(RSS/n) + 2k`, which only reflects the deviance part of the log-likelihood. R’s `AIC(lm)` uses the full normal log-likelihood, so it adds the constant term `n*(1 + log(2*pi))` and counts sigma as a parameter.

With n = 32 and k = 4 (intercept, wt, hp, and sigma), my hand value 65.84027 + 32*(1 + log(2*pi)) = 156.6523, which matches AIC(lm_fit)

```{r}
n <- nrow(X)
k <- ncol(X)+1

AIC_formula <- n * log(RSS / n) + 2 * k
AIC_formula

AIC(lm_fit) #not the same as the calculated AIC

```

# 6 Model Selection

The “adult.csv” dataset can be found on Canvas (Files/Data) and collects census information paired with individuals’ salary and earnings. We want to explain/predict the binary response = NA., which represents whether an individual’s income is above \(50\)k, using all other columns as predictors. Therefore you will be using logistic regression (see glm function). More specifically, the goal is to estimate \(m = 5\) models per dimension, with dimensions going from 1 to \(p_{max}\) (i.e. \(p^* = 1,..., p_{max}\)). For this data we have \(p = 14\) variables (in addition to the response) so in the first dimension you will estimate all 14 one-variable models. Then, in the second dimension, we estimate \(m\) distinct randomly chosen two-variable models (each with distinct variable combinations in them), and so on for all other dimensions up to \(p_{max} = 5\).

For all dimensions, make sure you save:

- the variables in each model, with their regression coefficients and their p-values
- the AIC of each model
- a table representing the number of models each predictor is included in

We the want to store all these into a single object.

```{r}
#Load Data
adult_df <- read.csv("adult.csv", sep = ";")

#Fixes to data frame
adult_df <- adult_df[,-1]
adult_df$`NA.` <- as.factor(adult_df$`NA.`)

#List of all predictors
predictors <- setdiff(names(adult_df), "NA.")


#2nd diminsion random predictors 
all_combos2 <- combn(predictors, 2)
random_combos2 <- sample(all_combos2, 5)

#3rd diminsion random predictors 
all_combos3 <- combn(predictors, 3)
random_combos3 <- sample(all_combos3, 5)  

#4th diminsion random predictors 
all_combos4 <- combn(predictors, 4)
random_combos4 <- sample(all_combos4, 5)

#5th diminsion random predictors 
all_combos5 <- combn(predictors, 5)
random_combos5 <- sample(all_combos5, 5)  

#Set lists
model_info <- list()
best_models_info <- list()
predictor_count <- setNames(rep(0, length(predictors)), predictors)

#Modeling loop for 1 diminsion
for (var in predictors) {
  
  formula <- as.formula(paste("`NA.` ~", var))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[var]] <- list(
    variables = var,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 2 diminsion
for (vars in random_combos2) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p2_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 3 diminsion
for (vars in random_combos3) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p3_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 4 diminsion

for (vars in random_combos4) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p4_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 5 diminsion

for (vars in random_combos5) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p5_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}
```

## 6.1 Best model per dimension

We will use AIC to measure model performance (i.e. the lower the AIC, the better the model). Create a tidy data frame with one row per dimension \(p^*\) containing:

- predictors in the best model
- AIC value
- number of parameters

```{r}
#Best 1 dimension model
best_name <- names(which.min(sapply(model_info[1:length(predictors)], function(x) x$AIC)))
best_models_info$p1 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 2 dimension model
p2_names <- grep("^p2_", names(model_info))
best_name <- p2_names[which.min(sapply(model_info[p2_names], function(x) x$AIC))]
best_models_info$p2 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 3 dimension model
p3_names <- grep("^p3_", names(model_info))
best_name <- p3_names[which.min(sapply(model_info[p3_names], function(x) x$AIC))]
best_models_info$p3 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 4 dimension model
p4_names <- grep("^p4_", names(model_info))
best_name <- p4_names[which.min(sapply(model_info[p4_names], function(x) x$AIC))]
best_models_info$p4 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 5 dimension model
p5_names <- grep("^p5_", names(model_info))
best_name <- p5_names[which.min(sapply(model_info[p5_names], function(x) x$AIC))]
best_models_info$p5 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)
```

Create boxplots representing the distribution of AIC values across all dimensions 1 to 5.

```{r}
#Empty list to store AIC values
aic_list <- list()

#Extract AIC values for each diminsion
for (dim in 1:5) {
  if (dim == 1) {
    #1 diminsion
    model_names <- names(model_info)[1:length(predictors)]
  } else {
    #2-5 diminsion
    model_names <- grep(paste0("^p", dim, "_"), names(model_info))
  }
  
  aic_list[[dim]] <- sapply(model_info[model_names], function(x) x$AIC)
}

# Create boxplot
boxplot(aic_list,
        names = 1:5,
        main = "Distribution of AIC Values Across Dimensions",
        xlab = "Dimension",
        ylab = "AIC")
```
