---
title: "Hw1"
output: html_document
date: "2025-09-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# General Information

Logan Bolton ChatGPT Link: "https://chatgpt.com/share/68d4c5be-d4e0-800e-971d-5e981b7c0f8f"


# 2. Vectors & Factors
## 2.1 Named numeric vector, attributes

```{r}
wts <- c(10, 20, 30, 40, 50)
names(wts) <- c("a", "b", "c", "d", "e")

attr(wts, "units") <- "kg"

typeof(wts)
length(wts)
names(wts)
attributes(wts)
```

## 2.2 Special values & coercion

```{r}
v <- c(1, NA, NaN, Inf, "2")
typeof(v)
```

By default, R will coerce mixed data types of numbers and characters into only being characters. That's why `typeof(v)` returns the value "character".

```{r}
v_num <- as.numeric(v)
finite_vals <- v_num[is.finite(v_num)]
median_val <- median(finite_vals)
print(median_val)

v_clean <- v_num
v_clean[!is.finite(v_clean)] <- median_val
v_clean[is.na(v_clean)] <- median_val
v_clean
```
## 2.3 Factors

In the mtcars dataset, 0 is automatic and 1 is manual. In the displayed table, there are 19 automatics and 13 manuals.

```{r}
table(mtcars$am)
am_f <- factor(mtcars$am, 
               levels = c(0, 1), 
               labels = c("Automatic", "Manual"))
am_f
```
```{r}
am_f <- relevel(am_f, ref = "Manual")
levels(am_f)
table(am_f)

```
# 3 Data Frames & Coercion

## 3.1 Data frame construction
```{r}
id <- 1:6
group <- factor(c("A", "B", "A", "B", "A", "B"))
val <- c(10.5, 20.1, NA, 15.2, 30.0, 25.3)

df_small <- data.frame(id = id, group = group, val = val)
print(df_small)
sapply(df_small, class)

```


## 3.2 Coercion rules (vector vs data frame)

When we bind the dataframes together using `c(...)`, it converts everything to a double. This is because R stores the values internally as integers. Then when concatenating the values, everything is converted to the same numeric type and the group labels are lost. 

To prevent this from happening, we need to create a new dataframe that combines these vectors together with binding. This can be accomplished with the regular `data.frame(...)` function.

```{r}
concatted = c(df_small$group, df_small$val)
print(concatted)
typeof(concatted)
```
```{r}
df_pair <- data.frame(group = df_small$group,
                      val   = df_small$val)
print(df_pair)
print(typeof(df_pair))
```


# 4 Subsetting (12 pts)

```{r}
y <- mtcars$mpg
Xdf <- mtcars[, !(names(mtcars) %in% "mpg")]

head(y)
head(Xdf)
```

## 4.1 By position, name, logical

### By Position
```{r}
Xdf_pos <- Xdf[, 1:3]
head(Xdf_pos)
```

### By Name 
```{r}
Xdf_names <- Xdf[, c("hp", "wt", "qsec")]
head(Xdf_names)
```

### By Logical Condition 
```{r}
Xdf_wt3 <- Xdf[Xdf$wt > 3, ]
head(Xdf_wt3)
```
## 4.2 Negative indices via matching

```{r}
Xdf_drop <- Xdf[, -c(7, 8)]
head(Xdf_drop)

ncol(Xdf_drop)
```

## 4.3 List subsetting differences

```{r}
L <- list(
  y = y,           
  X = Xdf,        
  meta = list(n = nrow(Xdf))
)

class(L[["y"]])

class(L["y"])

class(L$y)

```




# 5 Matrices & Arrays (18 pts)

## 5.1 Model matrix with intercept

Pick predictors `S <- c("wt","hp")`.

Describe what the following does `X <- cbind(Intercept = 1, as.matrix(Xdf[, S, drop = FALSE]))`.
Confirm `dim(X)` and `colnames(X)`.

```{r}
S <- c("wt","hp")
#Binds a vector of 1s and the given vectors of S into a matrix  
X <- cbind(Intercept = 1, as.matrix(Xdf[, S, drop = FALSE]))  #drop=FALSE keeps the subset as a data frame
dim(X)
colnames(X)

```

## 5.2 Least Squares via matrix operations

Compute `β^=(X⊤X)−1X⊤y` and `y^=Xβ^` and `RSS=∥y^−y∥22`

Then fit an equivalent lm and verify that your computations are correct.

```{r}
#Compute beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta_hat

#Compute fitted values
y_hat <- X %*% beta_hat
head(y_hat) 

#Compute RSS
RSS <- sum((y - y_hat)^2)
RSS

#Fit model
lm_fit <- lm(mpg ~ wt + hp, data = mtcars)
summary(lm_fit) #matches beta hat
sum(resid(lm_fit)^2) #matches RSS
```

## 5.3 AIC

Use previous results to compute: `AIC=nlog(RSS/n)+2k`
with `k = number of estimated parameters incl. intercept and σ`. Compare to `AIC(lm_fit)`.
 
```{r}
n <- nrow(X)
k <- ncol(X)+1

AIC_formula <- n * log(RSS / n) + 2 * k
AIC_formula

AIC(lm_fit) #not the same as the calculated AIC

```

# 6 Model Selection

The “adult.csv” dataset can be found on Canvas (Files/Data) and collects census information paired with individuals’ salary and earnings. We want to explain/predict the binary response = NA., which represents whether an individual’s income is above \(50\)k, using all other columns as predictors. Therefore you will be using logistic regression (see glm function). More specifically, the goal is to estimate \(m = 5\) models per dimension, with dimensions going from 1 to \(p_{max}\) (i.e. \(p^* = 1,..., p_{max}\)). For this data we have \(p = 14\) variables (in addition to the response) so in the first dimension you will estimate all 14 one-variable models. Then, in the second dimension, we estimate \(m\) distinct randomly chosen two-variable models (each with distinct variable combinations in them), and so on for all other dimensions up to \(p_{max} = 5\).

For all dimensions, make sure you save:

- the variables in each model, with their regression coefficients and their p-values
- the AIC of each model
- a table representing the number of models each predictor is included in

We the want to store all these into a single object.

```{r}
#Load Data
adult_df <- read.csv("C:/Users/jaken/Downloads/adult.csv", sep = ";")

#Fixes to data frame
adult_df <- adult_df[,-1]
adult_df$`NA.` <- as.factor(adult_df$`NA.`)

#List of all predictors
predictors <- setdiff(names(adult_df), "NA.")


#2nd diminsion random predictors 
all_combos2 <- combn(predictors, 2)
random_combos2 <- sample(all_combos2, 5)

#3rd diminsion random predictors 
all_combos3 <- combn(predictors, 3)
random_combos3 <- sample(all_combos3, 5)  

#4th diminsion random predictors 
all_combos4 <- combn(predictors, 4)
random_combos4 <- sample(all_combos4, 5)

#5th diminsion random predictors 
all_combos5 <- combn(predictors, 5)
random_combos5 <- sample(all_combos5, 5)  

#Set lists
model_info <- list()
best_models_info <- list()
predictor_count <- setNames(rep(0, length(predictors)), predictors)

#Modeling loop for 1 diminsion
for (var in predictors) {
  
  formula <- as.formula(paste("`NA.` ~", var))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[var]] <- list(
    variables = var,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 2 diminsion
for (vars in random_combos2) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p2_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 3 diminsion
for (vars in random_combos3) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p3_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 4 diminsion

for (vars in random_combos4) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p4_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 5 diminsion

for (vars in random_combos5) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p5_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}
```

## 6.1 Best model per dimension

We will use AIC to measure model performance (i.e. the lower the AIC, the better the model). Create a tidy data frame with one row per dimension \(p^*\) containing:

- predictors in the best model
- AIC value
- number of parameters

```{r}
#Best 1 dimension model
best_name <- names(which.min(sapply(model_info[1:length(predictors)], function(x) x$AIC)))
best_models_info$p1 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 2 dimension model
p2_names <- grep("^p2_", names(model_info))
best_name <- p2_names[which.min(sapply(model_info[p2_names], function(x) x$AIC))]
best_models_info$p2 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 3 dimension model
p3_names <- grep("^p3_", names(model_info))
best_name <- p3_names[which.min(sapply(model_info[p3_names], function(x) x$AIC))]
best_models_info$p3 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 4 dimension model
p4_names <- grep("^p4_", names(model_info))
best_name <- p4_names[which.min(sapply(model_info[p4_names], function(x) x$AIC))]
best_models_info$p4 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 5 dimension model
p5_names <- grep("^p5_", names(model_info))
best_name <- p5_names[which.min(sapply(model_info[p5_names], function(x) x$AIC))]
best_models_info$p5 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)
```

Create boxplots representing the distribution of AIC values across all dimensions 1 to 5.

```{r}
#Empty list to store AIC values
aic_list <- list()

#Extract AIC values for each diminsion
for (dim in 1:5) {
  if (dim == 1) {
    #1 diminsion
    model_names <- names(model_info)[1:length(predictors)]
  } else {
    #2-5 diminsion
    model_names <- grep(paste0("^p", dim, "_"), names(model_info))
  }
  
  aic_list[[dim]] <- sapply(model_info[model_names], function(x) x$AIC)
}

# Create boxplot
boxplot(aic_list,
        names = 1:5,
        main = "Distribution of AIC Values Across Dimensions",
        xlab = "Dimension",
        ylab = "AIC")
```
