---
title: "Hw1"
output: html_document
date: "2025-09-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# General Information

Github Repo Link: https://github.com/R-4-Data-Science/HW1

Logan Bolton ChatGPT Link: https://chatgpt.com/share/68d4c5be-d4e0-800e-971d-5e981b7c0f8f

Jonah Kennedy ChatGPT Link: https://chatgpt.com/share/68d5676f-c670-8008-97f3-b9b5edb6e74e

# 2. Vectors & Factors
## 2.1 Named numeric vector, attributes

typeof(wts) returns a double vector, length(wts) returns 5, names(wts) shows a–e, and attributes(wts) confirms the custom units = "kg" attribute.

```{r}
wts <- c(10, 20, 30, 40, 50)
names(wts) <- c("a", "b", "c", "d", "e")

attr(wts, "units") <- "kg"

typeof(wts)
length(wts)
names(wts)
attributes(wts)
```

## 2.2 Special values & coercion

By default, R will coerce mixed data types of numbers and characters into only being characters. That's why `typeof(v)` returns the value "character".

```{r}
v <- c(1, NA, NaN, Inf, "2")
typeof(v)
```

as.numeric(v) makes numeric values, median(finite_vals) is 1.5, and replacing non-finite entries yields v_clean where NA, NaN, and Inf are all set to 1.5.

```{r}
v_num <- as.numeric(v)
finite_vals <- v_num[is.finite(v_num)]
median_val <- median(finite_vals)
print(median_val)

v_clean <- v_num
v_clean[!is.finite(v_clean)] <- median_val
v_clean[is.na(v_clean)] <- median_val
v_clean
```
## 2.3 Factors

In the mtcars dataset, 0 is automatic and 1 is manual. In the displayed table, there are 19 automatics and 13 manuals.

```{r}
table(mtcars$am)
am_f <- factor(mtcars$am, 
               levels = c(0, 1), 
               labels = c("Automatic", "Manual"))
am_f
```
```{r}
am_f <- relevel(am_f, ref = "Manual")
levels(am_f)
table(am_f)

```
# 3 Data Frames & Coercion
## 3.1 Data frame construction

print(df_small) shows the intended columns. sapply(df_small, class) confirms that id is an integer, group is a factor, and val is double with one NA value.

```{r}
id <- 1:6
group <- factor(c("A", "B", "A", "B", "A", "B"))
val <- c(10.5, 20.1, NA, 15.2, 30.0, 25.3)

df_small <- data.frame(id = id, group = group, val = val)
print(df_small)
sapply(df_small, class)

```


## 3.2 Coercion rules (vector vs data frame)

When we bind the dataframes together using `c(...)`, it converts everything to a double. This is because R stores the values internally as integers. Then when concatenating the values, everything is converted to the same numeric type and the group labels are lost. 

To prevent this from happening, we can create a new dataframe that combines these vectors together with binding. This can be accomplished with the regular `data.frame(...)` function.

```{r}
concatted = c(df_small$group, df_small$val)
print(concatted)
typeof(concatted)
```
```{r}
df_pair <- data.frame(group = df_small$group,
                      val   = df_small$val)
print(df_pair)
print(typeof(df_pair))
```


# 4 Subsetting (12 pts)

`head(y)` prints the first six mpg values. `head(Xdf)` shows all other columns in a data frame.

```{r}
y <- mtcars$mpg
Xdf <- mtcars[, !(names(mtcars) %in% "mpg")]

head(y)
head(Xdf)
```

## 4.1 By position, name, logical

`Xdf[, 1:3]` selects the first three columns by position.

`Xdf[, c("hp","wt","qsec")]` selects named columns in that order.

`Xdf[Xdf$wt > 3, ]` filters rows with weight greater than 3.

### By Position
```{r}
Xdf_pos <- Xdf[, 1:3]
head(Xdf_pos)
```

### By Name 
```{r}
Xdf_names <- Xdf[, c("hp", "wt", "qsec")]
head(Xdf_names)
```

### By Logical Condition 
```{r}
Xdf_wt3 <- Xdf[Xdf$wt > 3, ]
head(Xdf_wt3)
```
## 4.2 Negative indices via matching

Dropping vs and am leaves an 8-column data frame, which is confirmed by `ncol(Xdf_drop)`.

```{r}
Xdf_drop <- Xdf[, -match(c("vs","am"), names(Xdf))]
head(Xdf_drop)

ncol(Xdf_drop)
```

## 4.3 List subsetting differences

L[["y"]] returns the object itself (a numeric vector). L["y"] returns a one-element list that still wraps the vector. L$y is equivalent to L[["y"]] for a named element.

```{r}
L <- list(
  y = y,           
  X = Xdf,        
  meta = list(n = nrow(Xdf))
)

class(L[["y"]])

class(L["y"])

class(L$y)

```




# 5 Matrices & Arrays (18 pts)

## 5.1 Model matrix with intercept

`cbind(Intercept = 1, as.matrix(...))` creates a model matrix with a column of ones plus wt and hp; `dim(X)` is 32×3 and `colnames(X)` confirms the column names.

```{r}
S <- c("wt","hp")
#Binds a vector of 1s and the given vectors of S into a matrix  
X <- cbind(Intercept = 1, as.matrix(Xdf[, S, drop = FALSE]))  #drop=FALSE keeps the subset as a data frame
dim(X)
colnames(X)

```

## 5.2 Least Squares via matrix operations

`beta_hat` matches the lm coefficients, `y_hat` are the fitted values, and `RSS` equals `sum(resid(lm_fit)^2)`, which verifies the correctness of the matrix solution.

```{r}
#Compute beta hat
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta_hat

#Compute fitted values
y_hat <- X %*% beta_hat
head(y_hat) 

#Compute RSS
RSS <- sum((y - y_hat)^2)
RSS

#Fit model
lm_fit <- lm(mpg ~ wt + hp, data = mtcars)
summary(lm_fit) #matches beta hat
sum(resid(lm_fit)^2) #matches RSS
```

## 5.3 AIC

We first used the simplified Gaussian AIC, `n*log(RSS/n) + 2k`, which only reflects the deviance part of the log-likelihood. R’s `AIC(lm)` uses the full normal log-likelihood, so it adds the constant term `n*(1 + log(2*pi))` and counts sigma as a parameter.

With n = 32 and k = 4 (intercept, wt, hp, and sigma), my hand value 65.84027 + 32*(1 + log(2*pi)) = 156.6523, which matches AIC(lm_fit)

```{r}
n <- nrow(X)
k <- ncol(X)+1

AIC_formula <- n * log(RSS / n) + 2 * k
AIC_formula

AIC(lm_fit) #not the same as the calculated AIC

```

# 6 Model Selection

```{r}
#Load Data
adult_df <- read.csv("adult.csv", sep = ";")

#Fixes to data frame
adult_df <- adult_df[,-1]
adult_df$`NA.` <- as.factor(adult_df$`NA.`)

#List of all predictors
predictors <- setdiff(names(adult_df), "NA.")


#2nd diminsion random predictors 
all_combos2 <- combn(predictors, 2, simplify = FALSE)
random_combos2 <- sample(all_combos2, 5)

#3rd diminsion random predictors 
all_combos3 <- combn(predictors, 3, simplify = FALSE)
random_combos3 <- sample(all_combos3, 5)  

#4th diminsion random predictors 
all_combos4 <- combn(predictors, 4, simplify = FALSE)
random_combos4 <- sample(all_combos4, 5)

#5th diminsion random predictors 
all_combos5 <- combn(predictors, 5, simplify = FALSE)
random_combos5 <- sample(all_combos5, 5)  

#Set lists
model_info <- list()
best_models_info <- list()
predictor_count <- setNames(rep(0, length(predictors)), predictors)

#Modeling loop for 1 diminsion
for (var in predictors) {
  
  formula <- as.formula(paste("`NA.` ~", var))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[var]] <- list(
    variables = var,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 2 diminsion
for (vars in random_combos2) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p2_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 3 diminsion
for (vars in random_combos3) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p3_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 4 diminsion

for (vars in random_combos4) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p4_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}

#Modeling loop for 5 diminsion

for (vars in random_combos5) {
  model_name <- paste(vars, collapse = "_")
  formula <- as.formula(paste("`NA.` ~", paste(vars, collapse = " + ")))
  model <- glm(formula, data = adult_df, family = binomial)
  
  #Save info and update counts
  coeffs <- summary(model)$coefficients
  model_info[[paste0("p5_", model_name)]] <- list(
    variables = vars,
    coefficients = coeffs[, "Estimate"],
    p_values = coeffs[, "Pr(>|z|)"],
    AIC = AIC(model)
  )
  
  for (var in vars) predictor_count[var] <- predictor_count[var] + 1
}
```

## 6.1 Best model per dimension

For each dimension (1–5), we identified the model with the lowest AIC. The best one-variable model was based on relationship; higher-dimensional best models typically included variables like education.num, hours.per.week, and sex. 

```{r}
#Best 1 dimension model
best_name <- names(which.min(sapply(model_info[1:length(predictors)], function(x) x$AIC)))
best_models_info$p1 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 2 dimension model
p2_names <- grep("^p2_", names(model_info))
best_name <- p2_names[which.min(sapply(model_info[p2_names], function(x) x$AIC))]
best_models_info$p2 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 3 dimension model
p3_names <- grep("^p3_", names(model_info))
best_name <- p3_names[which.min(sapply(model_info[p3_names], function(x) x$AIC))]
best_models_info$p3 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 4 dimension model
p4_names <- grep("^p4_", names(model_info))
best_name <- p4_names[which.min(sapply(model_info[p4_names], function(x) x$AIC))]
best_models_info$p4 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

#Best 5 dimension model
p5_names <- grep("^p5_", names(model_info))
best_name <- p5_names[which.min(sapply(model_info[p5_names], function(x) x$AIC))]
best_models_info$p5 <- list(
  predictors = model_info[[best_name]]$variables,
  AIC = model_info[[best_name]]$AIC,
  n_params = length(model_info[[best_name]]$coefficients)
)

best_df <- do.call(rbind, lapply(names(best_models_info), function(dim) {
  data.frame(
    dimension = dim,
    predictors = paste(best_models_info[[dim]]$predictors, collapse = ", "),
    AIC = best_models_info[[dim]]$AIC,
    n_params = best_models_info[[dim]]$n_params
  )
}))

best_df
```

The distributions show that higher-dimensional models generally achieve lower AIC values, indicating better fit, though variation exists due to the random selection of predictors in dimensions 2–5.

```{r}
#Empty list to store AIC values
aic_list <- list()

#Extract AIC values for each diminsion
for (dim in 1:5) {
  if (dim == 1) {
    #1 diminsion
    model_names <- names(model_info)[1:length(predictors)]
  } else {
    #2-5 diminsion
    model_names <- grep(paste0("^p", dim, "_"), names(model_info))
  }
  
  aic_list[[dim]] <- sapply(model_info[model_names], function(x) x$AIC)
}

# Create boxplot
boxplot(aic_list,
        names = 1:5,
        main = "Distribution of AIC Values Across Dimensions",
        xlab = "Dimension",
        ylab = "AIC")
```

```{r}
predictor_count_tbl <- data.frame(
  predictor = names(predictor_count),
  count = as.integer(predictor_count),
  row.names = NULL
)
predictor_count_tbl <- predictor_count_tbl[order(-predictor_count_tbl$count,
                                                 predictor_count_tbl$predictor),
                                           ]

knitr::kable(predictor_count_tbl,
             caption = "How often each predictor appears across all fitted models",
             align = c("l","r"),
             digits = 0)
```

```{r}
get_dim <- function(nm) {
  if (grepl("^p\\d+_", nm)) as.integer(sub("^p(\\d+)_.*", "\\1", nm)) else 1L
}

per_model_tbl <- do.call(
  rbind,
  lapply(names(model_info), function(nm) {
    info <- model_info[[nm]]
    data.frame(
      model_id   = nm,
      dimension  = get_dim(nm),
      predictors = paste(info$variables, collapse = ", "),
      AIC        = as.numeric(info$AIC),
      n_params   = length(info$coefficients),
      stringsAsFactors = FALSE
    )
  })
)

# Order by dimension then AIC
per_model_tbl <- per_model_tbl[order(per_model_tbl$dimension, per_model_tbl$AIC), ]

knitr::kable(per_model_tbl,
             caption = "Per-model summary: predictors, AIC, and parameter count",
             align = c("l","r","l","r","r"),
             digits = 2)
```
